{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9069693,"sourceType":"datasetVersion","datasetId":5470557},{"sourceId":9062453,"sourceType":"datasetVersion","datasetId":5465213}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!pip install datasets transformers torch accelerate sacremoses sacrebleu tmx --quiet\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:17:23.692485Z","iopub.execute_input":"2024-07-30T21:17:23.692870Z","iopub.status.idle":"2024-07-30T21:17:43.420303Z","shell.execute_reply.started":"2024-07-30T21:17:23.692840Z","shell.execute_reply":"2024-07-30T21:17:43.418805Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom bs4 import BeautifulSoup\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict, load_dataset, load_metric\nfrom transformers import MarianTokenizer, MarianMTModel, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, AutoTokenizer\nimport numpy as np\nimport torch, os","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:17:43.422789Z","iopub.execute_input":"2024-07-30T21:17:43.423131Z","iopub.status.idle":"2024-07-30T21:18:03.017197Z","shell.execute_reply.started":"2024-07-30T21:17:43.423099Z","shell.execute_reply":"2024-07-30T21:18:03.016392Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-30 21:17:52.586278: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-30 21:17:52.586419: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-30 21:17:52.714107: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Extract the data and add it to DataFrame","metadata":{}},{"cell_type":"code","source":"# Read the TMX file\ntmx_file = '/kaggle/input/tmx-gz/en-gu.tmx'\nwith open(tmx_file, 'r', encoding='utf-8') as f:\n    tmx_content = f.read()\n\n# Parse the TMX content\nsoup = BeautifulSoup(tmx_content, 'lxml')\n\n# Extract English and Gujarati translations\ntranslations = []\nfor tu in soup.find_all('tu'):\n    en_text = None\n    gu_text = None\n    for tuv in tu.find_all('tuv'):\n        lang = tuv['xml:lang']\n        seg = tuv.find('seg').text\n        if lang == 'en':\n            en_text = seg\n        elif lang == 'gu':\n            gu_text = seg\n    if en_text and gu_text:\n        translations.append({'English': en_text, 'Gujarati': gu_text})\n\n# Create DataFrame\ndf = pd.DataFrame(translations)\n\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:18:03.023658Z","iopub.execute_input":"2024-07-30T21:18:03.023983Z","iopub.status.idle":"2024-07-30T21:18:04.392108Z","shell.execute_reply.started":"2024-07-30T21:18:03.023938Z","shell.execute_reply":"2024-07-30T21:18:04.390992Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                             English  \\\n0  Anna Hazare has heavily criticized the Prime m...   \n1  Four years have passed but the government is a...   \n2  He added that regarding the Lokpal election, h...   \n3  Anna Hazare wrote a letter to PM Modi on Thurs...   \n4  Anna wrote that on 16th August 2011, for the e...   \n\n                                            Gujarati  \n0  અન્ના હજારેએ વડાપ્રધાન પર પ્રહાર કરતા કહ્યું ક...  \n1  પરંતુ ચાર વર્ષ વીતી ગયા પણ સરકાર કોઈ ના કોઈ કા...  \n2  તેમણે કહ્યું કે તે લોકપાલ નિયુક્તને લઈને બીજી ...  \n3  અન્ના હજારેએ વડાપ્રધાન મોદીને ગુરુવારે પત્ર લખ...  \n4  અન્નાએ લખ્યું કે, લોકપાલ અને લોકાયુક્તની નિયુક...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English</th>\n      <th>Gujarati</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Anna Hazare has heavily criticized the Prime m...</td>\n      <td>અન્ના હજારેએ વડાપ્રધાન પર પ્રહાર કરતા કહ્યું ક...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Four years have passed but the government is a...</td>\n      <td>પરંતુ ચાર વર્ષ વીતી ગયા પણ સરકાર કોઈ ના કોઈ કા...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>He added that regarding the Lokpal election, h...</td>\n      <td>તેમણે કહ્યું કે તે લોકપાલ નિયુક્તને લઈને બીજી ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Anna Hazare wrote a letter to PM Modi on Thurs...</td>\n      <td>અન્ના હજારેએ વડાપ્રધાન મોદીને ગુરુવારે પત્ર લખ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Anna wrote that on 16th August 2011, for the e...</td>\n      <td>અન્નાએ લખ્યું કે, લોકપાલ અને લોકાયુક્તની નિયુક...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Convert Dataset to Model Suitable Format","metadata":{}},{"cell_type":"code","source":"train_df, val_df = train_test_split(df, test_size=0.1)\n# Create Hugging Face Datasets from the DataFrames\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Combine into a DatasetDict\ndataset = DatasetDict({\n    'train': train_dataset,\n    'validation': val_dataset\n})","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:18:04.393710Z","iopub.execute_input":"2024-07-30T21:18:04.394164Z","iopub.status.idle":"2024-07-30T21:18:04.448012Z","shell.execute_reply.started":"2024-07-30T21:18:04.394122Z","shell.execute_reply":"2024-07-30T21:18:04.447160Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:18:04.449181Z","iopub.execute_input":"2024-07-30T21:18:04.449486Z","iopub.status.idle":"2024-07-30T21:18:04.456195Z","shell.execute_reply.started":"2024-07-30T21:18:04.449460Z","shell.execute_reply":"2024-07-30T21:18:04.455153Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['English', 'Gujarati', '__index_level_0__'],\n        num_rows: 3610\n    })\n    validation: Dataset({\n        features: ['English', 'Gujarati', '__index_level_0__'],\n        num_rows: 402\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load the Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"model_name = '/kaggle/input/finetuned-opusmt-en-to-hi-model'\ntokenizer = MarianTokenizer.from_pretrained(model_name)\n\n# Define a function to tokenize the dataset\ndef tokenize_function(examples):\n    inputs = examples['English']\n    targets = examples['Gujarati']\n    model_inputs = tokenizer(inputs, text_target=targets, truncation=True, padding=\"max_length\", max_length=128)\n    return model_inputs\n\n# Tokenize the datasets\ntokenized_datasets = dataset.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:18:04.457671Z","iopub.execute_input":"2024-07-30T21:18:04.457962Z","iopub.status.idle":"2024-07-30T21:18:10.751617Z","shell.execute_reply.started":"2024-07-30T21:18:04.457928Z","shell.execute_reply":"2024-07-30T21:18:10.750652Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3610 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8929123d6aaf411e97e58fc046ba2bef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/402 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74b30ffda2534bf8a347aa65a5974ada"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:18:10.753443Z","iopub.execute_input":"2024-07-30T21:18:10.753864Z","iopub.status.idle":"2024-07-30T21:18:10.760403Z","shell.execute_reply.started":"2024-07-30T21:18:10.753828Z","shell.execute_reply":"2024-07-30T21:18:10.759403Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['English', 'Gujarati', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 3610\n    })\n    validation: Dataset({\n        features: ['English', 'Gujarati', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 402\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:18:10.761958Z","iopub.execute_input":"2024-07-30T21:18:10.762374Z","iopub.status.idle":"2024-07-30T21:18:10.819582Z","shell.execute_reply.started":"2024-07-30T21:18:10.762319Z","shell.execute_reply":"2024-07-30T21:18:10.818390Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# Load the model\nmodel = MarianMTModel.from_pretrained(model_name)\nmodel.to(device)\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir='./results',\n    evaluation_strategy='epoch',\n    learning_rate=2e-5,\n    warmup_steps=500,  # Adjust warmup steps\n    gradient_accumulation_steps=2,  # Adjust gradient accumulation steps\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    save_total_limit=3,\n    predict_with_generate=True,\n    fp16=True\n)\n\n# Define the data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n# Define a function to compute metrics\nmetric = load_metric('sacrebleu', trust_remote_code=True)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Compute BLEU score\n    result = metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n    result = {\"bleu\": result[\"score\"]}\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    return result\n\n# Create the Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:18:10.822516Z","iopub.execute_input":"2024-07-30T21:18:10.822894Z","iopub.status.idle":"2024-07-30T21:18:16.435580Z","shell.execute_reply.started":"2024-07-30T21:18:10.822851Z","shell.execute_reply":"2024-07-30T21:18:16.434659Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_35/1169788528.py:24: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric('sacrebleu', trust_remote_code=True)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64133e98c5f14c57b5d5ada528f9925e"}},"metadata":{}}]},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:18:16.436756Z","iopub.execute_input":"2024-07-30T21:18:16.437033Z","iopub.status.idle":"2024-07-30T21:38:28.858641Z","shell.execute_reply.started":"2024-07-30T21:18:16.437009Z","shell.execute_reply":"2024-07-30T21:38:28.857710Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [560/560 20:08, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>4.703207</td>\n      <td>0.235085</td>\n      <td>52.965174</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.844223</td>\n      <td>7.632196</td>\n      <td>65.669154</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.750255</td>\n      <td>9.466346</td>\n      <td>65.522388</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.696945</td>\n      <td>10.809743</td>\n      <td>65.838308</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.616900</td>\n      <td>0.657723</td>\n      <td>12.827876</td>\n      <td>66.626866</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.616900</td>\n      <td>0.650156</td>\n      <td>13.094781</td>\n      <td>65.557214</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=560, training_loss=1.51349926676069, metrics={'train_runtime': 1211.3406, 'train_samples_per_second': 29.802, 'train_steps_per_second': 0.462, 'total_flos': 1213086887313408.0, 'train_loss': 1.51349926676069, 'epoch': 9.91150442477876})"},"metadata":{}}]},{"cell_type":"code","source":"# Save directory\nsave_directory = './finetuned-opusmt-en-hi-gu'\n\n# Save model and tokenizer\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)\n\nprint(f\"Model saved to {save_directory}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:38:28.859921Z","iopub.execute_input":"2024-07-30T21:38:28.860255Z","iopub.status.idle":"2024-07-30T21:38:29.716580Z","shell.execute_reply.started":"2024-07-30T21:38:28.860226Z","shell.execute_reply":"2024-07-30T21:38:29.715397Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","output_type":"stream"},{"name":"stdout","text":"Model saved to ./finetuned-opusmt-en-hi-gu\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Check out Translations of the Finetuned Model","metadata":{}},{"cell_type":"code","source":"# Load your model and tokenizer\nmodel_name =save_directory\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Example English sentence to translate\nenglish_sentence = \"The thief ran away from the scene\"\n\n# Tokenize the input sentence\ninputs = tokenizer(english_sentence, return_tensors=\"pt\").to(device)\n\n# Generate translation\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n\n# Decode the generated tokens\ntranslated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(f\"English: {english_sentence}\")\nprint(f\"Gujarati Translation: {translated_sentence}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:53:07.084880Z","iopub.execute_input":"2024-07-30T21:53:07.085973Z","iopub.status.idle":"2024-07-30T21:53:09.779068Z","shell.execute_reply.started":"2024-07-30T21:53:07.085936Z","shell.execute_reply":"2024-07-30T21:53:09.778007Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"English: The thief ran away from the scene\nGujarati Translation: ચોર સ્થિતિથી ભાગી ગયો\n","output_type":"stream"}]}]}