{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!pip install datasets transformers torch accelerate sacremoses sacrebleu --quiet","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:02:20.315443Z","iopub.execute_input":"2024-07-30T14:02:20.316321Z","iopub.status.idle":"2024-07-30T14:02:34.814067Z","shell.execute_reply.started":"2024-07-30T14:02:20.316287Z","shell.execute_reply":"2024-07-30T14:02:34.813015Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, load_metric\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\nfrom transformers import MarianTokenizer, MarianMTModel, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\nimport numpy as np\nimport torch, os","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:02:34.816090Z","iopub.execute_input":"2024-07-30T14:02:34.816405Z","iopub.status.idle":"2024-07-30T14:02:52.491623Z","shell.execute_reply.started":"2024-07-30T14:02:34.816378Z","shell.execute_reply":"2024-07-30T14:02:52.490754Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-30 14:02:43.141016: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-30 14:02:43.141140: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-30 14:02:43.275238: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load the dataset","metadata":{}},{"cell_type":"code","source":"raw_dataset = load_dataset('ai4bharat/samanantar', 'te', split='train', streaming=True, trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:02:52.496449Z","iopub.execute_input":"2024-07-30T14:02:52.496724Z","iopub.status.idle":"2024-07-30T14:02:59.038152Z","shell.execute_reply.started":"2024-07-30T14:02:52.496700Z","shell.execute_reply":"2024-07-30T14:02:59.037239Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/3.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fc30ba08c96439fa07ee11c40fee056"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/6.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1a52db3bcd34b7e86c473d9cc34834c"}},"metadata":{}}]},{"cell_type":"code","source":"limited_data = raw_dataset.take(200000)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:02:59.039313Z","iopub.execute_input":"2024-07-30T14:02:59.039616Z","iopub.status.idle":"2024-07-30T14:02:59.044589Z","shell.execute_reply.started":"2024-07-30T14:02:59.039592Z","shell.execute_reply":"2024-07-30T14:02:59.043692Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"limited_data","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:02:59.045731Z","iopub.execute_input":"2024-07-30T14:02:59.045996Z","iopub.status.idle":"2024-07-30T14:02:59.058086Z","shell.execute_reply.started":"2024-07-30T14:02:59.045974Z","shell.execute_reply":"2024-07-30T14:02:59.057273Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"IterableDataset({\n    features: ['idx', 'src', 'tgt'],\n    n_shards: 1\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Convert the IterableDataset to a list\nlimited_data_list = list(limited_data)\n\n# Create a Dataset from the list\nlimited_data = Dataset.from_list(limited_data_list)\n\n# Create a DatasetDict\ndataset_dict = DatasetDict({\"train\": limited_data})\n\n# Verify the first example to ensure conversion was successful\nprint(dataset_dict[\"train\"][0])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:02:59.059261Z","iopub.execute_input":"2024-07-30T14:02:59.059979Z","iopub.status.idle":"2024-07-30T14:03:50.503943Z","shell.execute_reply.started":"2024-07-30T14:02:59.059954Z","shell.execute_reply":"2024-07-30T14:03:50.503035Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{'idx': 0, 'src': 'Have you heard about Foie gras?', 'tgt': '‡∞á‡∞ï ‡∞´‡±ç‡∞∞‡±Ç‡∞ü‡±ç ‡∞´‡±ç‡∞≤‡±à‡∞∏‡±ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞µ‡∞ø‡∞®‡±ç‡∞®‡∞æ‡∞∞‡∞æ?'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert the dataset to a Pandas DataFrame\ntrain_df = dataset_dict[\"train\"].to_pandas()\n\n# Rename the columns\ntrain_df = train_df.rename(columns={\"src\": \"en\", \"tgt\": \"te\"})\n\n# Drop the 'idx' column if it is not needed\ntrain_df = train_df.drop(columns=[\"idx\"])\n\n# Display the first few rows to verify\ntrain_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:03:50.505329Z","iopub.execute_input":"2024-07-30T14:03:50.505765Z","iopub.status.idle":"2024-07-30T14:03:50.792485Z","shell.execute_reply.started":"2024-07-30T14:03:50.505733Z","shell.execute_reply":"2024-07-30T14:03:50.791568Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                  en  \\\n0                    Have you heard about Foie gras?   \n1                I never thought of acting in films.   \n2                                 Installed Software   \n3  A case has been registered under Sections 302 ...   \n4      Of this, 10 people succumbed to the injuries.   \n\n                                                  te  \n0            ‡∞á‡∞ï ‡∞´‡±ç‡∞∞‡±Ç‡∞ü‡±ç ‡∞´‡±ç‡∞≤‡±à‡∞∏‡±ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞µ‡∞ø‡∞®‡±ç‡∞®‡∞æ‡∞∞‡∞æ?  \n1      ‡∞∏‡±Ç‡∞∞‡±ç‡∞Ø ‡∞∏‡∞ø‡∞®‡∞ø‡∞Æ‡∞æ‡∞≤‡±ç‡∞≤‡±ã ‡∞®‡∞ü‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡∞®‡∞ø ‡∞é‡∞™‡±ç‡∞™‡±Å‡∞°‡±Ç ‡∞Ö‡∞®‡±Å‡∞ï‡±ã‡∞≤‡±á‡∞¶‡±Å.  \n2                           ‡∞∏‡±ç‡∞•‡∞æ‡∞™‡∞ø‡∞Ç‡∞ö‡∞¨‡∞°‡∞ø‡∞® ‡∞∏‡∞æ‡∞´‡±ç‡∞ü‡±ç‚Äç‡∞µ‡±á‡∞∞‡±ç  \n3  ‡∞®‡∞ø‡∞Ç‡∞¶‡∞ø‡∞§‡±Å‡∞≤‡∞™‡±à ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç 376 ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å 302‡∞≤ ‡∞ï‡∞ø‡∞Ç‡∞¶ ‡∞ï‡±á‡∞∏‡±Å ‡∞®‡∞Æ...  \n4                ‡∞Ö‡∞Ç‡∞¶‡±Å‡∞≤‡±ã 10 ‡∞Æ‡∞Ç‡∞¶‡∞ø ‡∞§‡±Ä‡∞µ‡±ç‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞ó‡∞æ‡∞Ø‡∞™‡∞°‡±ç‡∞°‡∞æ‡∞∞‡±Å.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>te</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Have you heard about Foie gras?</td>\n      <td>‡∞á‡∞ï ‡∞´‡±ç‡∞∞‡±Ç‡∞ü‡±ç ‡∞´‡±ç‡∞≤‡±à‡∞∏‡±ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞µ‡∞ø‡∞®‡±ç‡∞®‡∞æ‡∞∞‡∞æ?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I never thought of acting in films.</td>\n      <td>‡∞∏‡±Ç‡∞∞‡±ç‡∞Ø ‡∞∏‡∞ø‡∞®‡∞ø‡∞Æ‡∞æ‡∞≤‡±ç‡∞≤‡±ã ‡∞®‡∞ü‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡∞®‡∞ø ‡∞é‡∞™‡±ç‡∞™‡±Å‡∞°‡±Ç ‡∞Ö‡∞®‡±Å‡∞ï‡±ã‡∞≤‡±á‡∞¶‡±Å.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Installed Software</td>\n      <td>‡∞∏‡±ç‡∞•‡∞æ‡∞™‡∞ø‡∞Ç‡∞ö‡∞¨‡∞°‡∞ø‡∞® ‡∞∏‡∞æ‡∞´‡±ç‡∞ü‡±ç‚Äç‡∞µ‡±á‡∞∞‡±ç</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A case has been registered under Sections 302 ...</td>\n      <td>‡∞®‡∞ø‡∞Ç‡∞¶‡∞ø‡∞§‡±Å‡∞≤‡∞™‡±à ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç 376 ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å 302‡∞≤ ‡∞ï‡∞ø‡∞Ç‡∞¶ ‡∞ï‡±á‡∞∏‡±Å ‡∞®‡∞Æ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Of this, 10 people succumbed to the injuries.</td>\n      <td>‡∞Ö‡∞Ç‡∞¶‡±Å‡∞≤‡±ã 10 ‡∞Æ‡∞Ç‡∞¶‡∞ø ‡∞§‡±Ä‡∞µ‡±ç‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞ó‡∞æ‡∞Ø‡∞™‡∞°‡±ç‡∞°‡∞æ‡∞∞‡±Å.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Prepare the dataset for the model","metadata":{}},{"cell_type":"code","source":"train_df, val_df = train_test_split(train_df, test_size=0.1)\n\n# Create Hugging Face Datasets from the DataFrames\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Combine into a DatasetDict\ndataset = DatasetDict({\n    'train': train_dataset,\n    'validation': val_dataset\n})","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:03:50.793856Z","iopub.execute_input":"2024-07-30T14:03:50.794163Z","iopub.status.idle":"2024-07-30T14:03:51.276692Z","shell.execute_reply.started":"2024-07-30T14:03:50.794137Z","shell.execute_reply":"2024-07-30T14:03:51.275924Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:03:51.279359Z","iopub.execute_input":"2024-07-30T14:03:51.279644Z","iopub.status.idle":"2024-07-30T14:03:51.285437Z","shell.execute_reply.started":"2024-07-30T14:03:51.279620Z","shell.execute_reply":"2024-07-30T14:03:51.284504Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['en', 'te', '__index_level_0__'],\n        num_rows: 180000\n    })\n    validation: Dataset({\n        features: ['en', 'te', '__index_level_0__'],\n        num_rows: 20000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load the Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"model_name = 'Helsinki-NLP/opus-mt-en-mul'\ntokenizer = MarianTokenizer.from_pretrained(model_name)\n\n# Define a function to tokenize the dataset\ndef tokenize_function(examples):\n    inputs = examples['en']\n    targets = examples['te']\n    model_inputs = tokenizer(inputs, text_target=targets, truncation=True, padding=\"max_length\", max_length=128)\n    return model_inputs\n\n# Tokenize the datasets\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:03:51.286621Z","iopub.execute_input":"2024-07-30T14:03:51.287495Z","iopub.status.idle":"2024-07-30T14:05:06.028130Z","shell.execute_reply.started":"2024-07-30T14:03:51.287469Z","shell.execute_reply":"2024-07-30T14:05:06.027259Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a86b13c1a0284fc1a0e9d47ed0e9ffab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/790k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"638a20df80eb4c058d147700f04c94ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/707k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"131067e688be45b7b21ababd0868989d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"961dec3d26a54023b6d2839c2c670b7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55a2e85116f3473f9366732dbe5eaa8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/180000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77cbf6cee1454b30aebc721a09c208fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23dc36971df2489c87a4efbed867aa17"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:05:06.029320Z","iopub.execute_input":"2024-07-30T14:05:06.029601Z","iopub.status.idle":"2024-07-30T14:05:06.035462Z","shell.execute_reply.started":"2024-07-30T14:05:06.029578Z","shell.execute_reply":"2024-07-30T14:05:06.034587Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['en', 'te', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 180000\n    })\n    validation: Dataset({\n        features: ['en', 'te', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 20000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:05:06.036530Z","iopub.execute_input":"2024-07-30T14:05:06.036788Z","iopub.status.idle":"2024-07-30T14:05:06.073469Z","shell.execute_reply.started":"2024-07-30T14:05:06.036765Z","shell.execute_reply":"2024-07-30T14:05:06.072643Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Load the model\nmodel = MarianMTModel.from_pretrained(model_name)\nmodel.to(device)\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir='./results',\n    evaluation_strategy='epoch',\n    learning_rate=2e-5,\n    warmup_steps=500,  \n    gradient_accumulation_steps=2,  \n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    save_total_limit=3,\n    predict_with_generate=True,\n    fp16=True\n)\n\n# Define the data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n# Define a function to compute metrics\nmetric = load_metric('sacrebleu', trust_remote_code=True)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Compute BLEU score\n    result = metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n    result = {\"bleu\": result[\"score\"]}\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    return result\n\n# Create the Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:05:06.074548Z","iopub.execute_input":"2024-07-30T14:05:06.074822Z","iopub.status.idle":"2024-07-30T14:05:18.520131Z","shell.execute_reply.started":"2024-07-30T14:05:06.074798Z","shell.execute_reply":"2024-07-30T14:05:18.519261Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/310M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8708d85922a94b70ae12ba8f2c7c48d2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aec7974d62d42beb1da638cc22bbdfb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_34/2219432604.py:24: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric('sacrebleu', trust_remote_code=True)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee65b61143764550bb771f7f61517dc6"}},"metadata":{}}]},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:05:18.523884Z","iopub.execute_input":"2024-07-30T14:05:18.524543Z","iopub.status.idle":"2024-07-30T15:21:11.722165Z","shell.execute_reply.started":"2024-07-30T14:05:18.524516Z","shell.execute_reply":"2024-07-30T15:21:11.721241Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5625/5625 1:15:51, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.452300</td>\n      <td>0.415945</td>\n      <td>6.227458</td>\n      <td>28.710100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5625, training_loss=0.5874648390028212, metrics={'train_runtime': 4552.1238, 'train_samples_per_second': 39.542, 'train_steps_per_second': 1.236, 'total_flos': 6101705687040000.0, 'train_loss': 0.5874648390028212, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Save the Finetuned Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"# Save directory\nsave_directory = './finetuned-opusmt-en-to-te'\n\n# Save model and tokenizer\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)\n\nprint(f\"Model saved to {save_directory}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T15:21:11.723533Z","iopub.execute_input":"2024-07-30T15:21:11.723988Z","iopub.status.idle":"2024-07-30T15:21:12.506799Z","shell.execute_reply.started":"2024-07-30T15:21:11.723955Z","shell.execute_reply":"2024-07-30T15:21:12.505920Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","output_type":"stream"},{"name":"stdout","text":"Model saved to ./finetuned-opusmt-en-to-te\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load the finetuned model and check out the translation","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\n# Load your model and tokenizer\nmodel_name =save_directory\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Example English sentence to translate\nenglish_sentence = \"My name is Varsha\"\n\n# Tokenize the input sentence\ninputs = tokenizer(english_sentence, return_tensors=\"pt\").to(device)\n\n# Generate translation\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n\n# Decode the generated tokens\ntranslated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(f\"English: {english_sentence}\")\nprint(f\"Telugu Translation: {translated_sentence}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T15:21:12.520437Z","iopub.execute_input":"2024-07-30T15:21:12.520740Z","iopub.status.idle":"2024-07-30T15:21:14.750202Z","shell.execute_reply.started":"2024-07-30T15:21:12.520715Z","shell.execute_reply":"2024-07-30T15:21:14.749280Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"English: My name is Varsha\nTelugu Translation: ‡∞®‡∞æ ‡∞™‡±á‡∞∞‡±Å ‡∞µ‡∞∞‡±ç‡∞∑\n","output_type":"stream"}]}]}