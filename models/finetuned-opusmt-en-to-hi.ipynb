{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8993046,"sourceType":"datasetVersion","datasetId":5416554},{"sourceId":8999214,"sourceType":"datasetVersion","datasetId":5420928},{"sourceId":8999221,"sourceType":"datasetVersion","datasetId":5420933},{"sourceId":9011218,"sourceType":"datasetVersion","datasetId":5429293}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!pip install datasets transformers torch accelerate sacremoses sacrebleu --quiet","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:54:27.172313Z","iopub.execute_input":"2024-07-29T19:54:27.172685Z","iopub.status.idle":"2024-07-29T19:54:39.348675Z","shell.execute_reply.started":"2024-07-29T19:54:27.172655Z","shell.execute_reply":"2024-07-29T19:54:39.347693Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset, load_metric\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\nfrom transformers import MarianTokenizer, MarianMTModel, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\nimport numpy as np\nimport torch, os","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:54:39.350830Z","iopub.execute_input":"2024-07-29T19:54:39.351124Z","iopub.status.idle":"2024-07-29T19:54:39.358975Z","shell.execute_reply.started":"2024-07-29T19:54:39.351099Z","shell.execute_reply":"2024-07-29T19:54:39.358094Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Load the dataset","metadata":{}},{"cell_type":"code","source":"raw_dataset = load_dataset('ai4bharat/samanantar', 'hi', split='train', streaming=True, trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:54:39.360268Z","iopub.execute_input":"2024-07-29T19:54:39.361006Z","iopub.status.idle":"2024-07-29T19:54:45.181302Z","shell.execute_reply.started":"2024-07-29T19:54:39.360973Z","shell.execute_reply":"2024-07-29T19:54:45.180264Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Take the first 200,000 rows\nlimited_data = raw_dataset.take(200000)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:54:45.183833Z","iopub.execute_input":"2024-07-29T19:54:45.184500Z","iopub.status.idle":"2024-07-29T19:54:45.189410Z","shell.execute_reply.started":"2024-07-29T19:54:45.184464Z","shell.execute_reply":"2024-07-29T19:54:45.188475Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"limited_data","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:54:45.190569Z","iopub.execute_input":"2024-07-29T19:54:45.190829Z","iopub.status.idle":"2024-07-29T19:54:45.205124Z","shell.execute_reply.started":"2024-07-29T19:54:45.190806Z","shell.execute_reply":"2024-07-29T19:54:45.204326Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"IterableDataset({\n    features: ['idx', 'src', 'tgt'],\n    n_shards: 1\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Convert the IterableDataset to a list\nlimited_data_list = list(limited_data)\n\n# Create a Dataset from the list\nlimited_data = Dataset.from_list(limited_data_list)\n\n# Create a DatasetDict\ndataset_dict = DatasetDict({\"train\": limited_data})\n\n# Verify the first example to ensure conversion was successful\nprint(dataset_dict[\"train\"][0])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:54:45.206200Z","iopub.execute_input":"2024-07-29T19:54:45.206471Z","iopub.status.idle":"2024-07-29T19:55:46.100884Z","shell.execute_reply.started":"2024-07-29T19:54:45.206449Z","shell.execute_reply":"2024-07-29T19:55:46.099812Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"{'idx': 0, 'src': \"However, Paes, who was partnering Australia's Paul Hanley, could only go as far as the quarterfinals where they lost to Bhupathi and Knowles\", 'tgt': 'आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert the dataset to a Pandas DataFrame\ntrain_df = dataset_dict[\"train\"].to_pandas()\n\n# Rename the columns\ntrain_df = train_df.rename(columns={\"src\": \"en\", \"tgt\": \"hi\"})\n\n# Drop the 'idx' column if it is not needed\ntrain_df = train_df.drop(columns=[\"idx\"])\n\n# Display the first few rows to verify\ntrain_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:55:46.102326Z","iopub.execute_input":"2024-07-29T19:55:46.102702Z","iopub.status.idle":"2024-07-29T19:55:46.522882Z","shell.execute_reply.started":"2024-07-29T19:55:46.102668Z","shell.execute_reply":"2024-07-29T19:55:46.521819Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                                                  en  \\\n0  However, Paes, who was partnering Australia's ...   \n1  Whosoever desires the reward of the world, wit...   \n2  The value of insects in the biosphere is enorm...   \n3  Mithali To Anchor Indian Team Against Australi...   \n4  After the assent of the Honble President on 8t...   \n\n                                                  hi  \n0  आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाल...  \n1  और जो शख्स (अपने आमाल का) बदला दुनिया ही में च...  \n2  जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि ...  \n3    आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को  \n4  8 सितम्‍बर, 2016 को माननीय राष्‍ट्रपति की स्‍व...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>hi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>However, Paes, who was partnering Australia's ...</td>\n      <td>आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाल...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Whosoever desires the reward of the world, wit...</td>\n      <td>और जो शख्स (अपने आमाल का) बदला दुनिया ही में च...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The value of insects in the biosphere is enorm...</td>\n      <td>जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Mithali To Anchor Indian Team Against Australi...</td>\n      <td>आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>After the assent of the Honble President on 8t...</td>\n      <td>8 सितम्‍बर, 2016 को माननीय राष्‍ट्रपति की स्‍व...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Prepare the dataset for the model","metadata":{}},{"cell_type":"code","source":"train_df, val_df = train_test_split(train_df, test_size=0.1)\n\n# Create Hugging Face Datasets from the DataFrames\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Combine into a DatasetDict\ndataset = DatasetDict({\n    'train': train_dataset,\n    'validation': val_dataset\n})","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:55:46.524111Z","iopub.execute_input":"2024-07-29T19:55:46.524431Z","iopub.status.idle":"2024-07-29T19:55:47.203268Z","shell.execute_reply.started":"2024-07-29T19:55:46.524404Z","shell.execute_reply":"2024-07-29T19:55:47.202326Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:55:47.204446Z","iopub.execute_input":"2024-07-29T19:55:47.204754Z","iopub.status.idle":"2024-07-29T19:55:47.210766Z","shell.execute_reply.started":"2024-07-29T19:55:47.204727Z","shell.execute_reply":"2024-07-29T19:55:47.209800Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['en', 'hi', '__index_level_0__'],\n        num_rows: 180000\n    })\n    validation: Dataset({\n        features: ['en', 'hi', '__index_level_0__'],\n        num_rows: 20000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load the Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"model_name = 'Helsinki-NLP/opus-mt-en-mul'\ntokenizer = MarianTokenizer.from_pretrained(model_name)\n\n# Define a function to tokenize the dataset\ndef tokenize_function(examples):\n    inputs = examples['en']\n    targets = examples['hi']\n    model_inputs = tokenizer(inputs, text_target=targets, truncation=True, padding=\"max_length\", max_length=128)\n    return model_inputs\n\n# Tokenize the datasets\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:55:47.213712Z","iopub.execute_input":"2024-07-29T19:55:47.213983Z","iopub.status.idle":"2024-07-29T19:57:21.199653Z","shell.execute_reply.started":"2024-07-29T19:55:47.213959Z","shell.execute_reply":"2024-07-29T19:57:21.198737Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/180000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6e0158040fa47589bb50f3e345c1077"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c0faa0d14a44f46a54bcdc50ab3d69a"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:57:21.200903Z","iopub.execute_input":"2024-07-29T19:57:21.201206Z","iopub.status.idle":"2024-07-29T19:57:21.206721Z","shell.execute_reply.started":"2024-07-29T19:57:21.201180Z","shell.execute_reply":"2024-07-29T19:57:21.206031Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['en', 'hi', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 180000\n    })\n    validation: Dataset({\n        features: ['en', 'hi', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 20000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:57:21.208078Z","iopub.execute_input":"2024-07-29T19:57:21.208423Z","iopub.status.idle":"2024-07-29T19:57:21.221380Z","shell.execute_reply.started":"2024-07-29T19:57:21.208398Z","shell.execute_reply":"2024-07-29T19:57:21.220436Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Load the model\nmodel = MarianMTModel.from_pretrained(model_name)\nmodel.to(device)\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir='./results',\n    evaluation_strategy='epoch',\n    learning_rate=2e-5,\n    warmup_steps=500,  \n    gradient_accumulation_steps=2,  \n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    save_total_limit=3,\n    predict_with_generate=True,\n    fp16=True\n)\n\n# Define the data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n# Define a function to compute metrics\nmetric = load_metric('sacrebleu', trust_remote_code=True)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Compute BLEU score\n    result = metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n    result = {\"bleu\": result[\"score\"]}\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    return result\n\n# Create the Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:57:21.223183Z","iopub.execute_input":"2024-07-29T19:57:21.223502Z","iopub.status.idle":"2024-07-29T19:57:23.973073Z","shell.execute_reply.started":"2024-07-29T19:57:21.223479Z","shell.execute_reply":"2024-07-29T19:57:23.972089Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:57:23.974185Z","iopub.execute_input":"2024-07-29T19:57:23.974474Z","iopub.status.idle":"2024-07-29T22:55:21.551719Z","shell.execute_reply.started":"2024-07-29T19:57:23.974449Z","shell.execute_reply":"2024-07-29T22:55:21.550792Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5624' max='5624' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5624/5624 2:57:56, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.666300</td>\n      <td>0.602311</td>\n      <td>14.266666</td>\n      <td>45.135550</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.620900</td>\n      <td>0.578757</td>\n      <td>15.235178</td>\n      <td>45.047350</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5624, training_loss=0.7583249880273855, metrics={'train_runtime': 10677.0962, 'train_samples_per_second': 33.717, 'train_steps_per_second': 0.527, 'total_flos': 1.2201241878724608e+16, 'train_loss': 0.7583249880273855, 'epoch': 1.9996444444444443})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Save the Finetuned Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"# Save directory\nsave_directory = './finetuned-opusmt-en-to-hi'\n\n# Save model and tokenizer\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)\n\nprint(f\"Model saved to {save_directory}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-29T22:55:21.552770Z","iopub.execute_input":"2024-07-29T22:55:21.553026Z","iopub.status.idle":"2024-07-29T22:55:22.339003Z","shell.execute_reply.started":"2024-07-29T22:55:21.553004Z","shell.execute_reply":"2024-07-29T22:55:22.338049Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","output_type":"stream"},{"name":"stdout","text":"Model saved to ./finetuned-opusmt-en-to-hi\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load the finetuned model and check out the translation","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\n\n# Load your model and tokenizer\nmodel_name =save_directory\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Example English sentence to translate\nenglish_sentence = \"My name is Varsha\"\n\n# Tokenize the input sentence\ninputs = tokenizer(english_sentence, return_tensors=\"pt\").to(device)\n\n# Generate translation\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n\n# Decode the generated tokens\ntranslated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(f\"English: {english_sentence}\")\nprint(f\"Hindi Translation: {translated_sentence}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T22:55:22.340176Z","iopub.execute_input":"2024-07-29T22:55:22.340484Z","iopub.status.idle":"2024-07-29T22:55:24.865810Z","shell.execute_reply.started":"2024-07-29T22:55:22.340458Z","shell.execute_reply":"2024-07-29T22:55:24.864870Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"English: My name is Varsha\nHindi Translation: मेरा नाम वर्षा है\n","output_type":"stream"}]}]}