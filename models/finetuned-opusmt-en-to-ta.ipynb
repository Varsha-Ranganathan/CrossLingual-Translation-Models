{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!pip install datasets transformers torch accelerate sacremoses sacrebleu --quiet","metadata":{"execution":{"iopub.status.busy":"2024-07-30T02:49:46.247846Z","iopub.execute_input":"2024-07-30T02:49:46.249126Z","iopub.status.idle":"2024-07-30T02:50:01.017571Z","shell.execute_reply.started":"2024-07-30T02:49:46.249081Z","shell.execute_reply":"2024-07-30T02:50:01.016325Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, load_metric\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\nfrom transformers import MarianTokenizer, MarianMTModel, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\nimport numpy as np\nimport torch, os","metadata":{"execution":{"iopub.status.busy":"2024-07-30T02:50:01.019922Z","iopub.execute_input":"2024-07-30T02:50:01.020402Z","iopub.status.idle":"2024-07-30T02:50:02.659496Z","shell.execute_reply.started":"2024-07-30T02:50:01.020363Z","shell.execute_reply":"2024-07-30T02:50:02.658506Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Load the dataset","metadata":{}},{"cell_type":"code","source":"raw_dataset = load_dataset('ai4bharat/samanantar', 'ta', split='train', streaming=True, trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T02:50:02.670505Z","iopub.execute_input":"2024-07-30T02:50:02.670905Z","iopub.status.idle":"2024-07-30T02:50:11.894604Z","shell.execute_reply.started":"2024-07-30T02:50:02.670873Z","shell.execute_reply":"2024-07-30T02:50:11.893780Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/3.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed3d60b3e2c5439890d17bd69d9c2a90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/6.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a048e56e78dd484a8dd76bc934cc3aa0"}},"metadata":{}}]},{"cell_type":"code","source":"# Take the first 100,000 rows\nlimited_data = raw_dataset.take(200000)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T02:50:11.895660Z","iopub.execute_input":"2024-07-30T02:50:11.896139Z","iopub.status.idle":"2024-07-30T02:50:11.900721Z","shell.execute_reply.started":"2024-07-30T02:50:11.896112Z","shell.execute_reply":"2024-07-30T02:50:11.899793Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"limited_data","metadata":{"execution":{"iopub.status.busy":"2024-07-30T02:50:11.901829Z","iopub.execute_input":"2024-07-30T02:50:11.902111Z","iopub.status.idle":"2024-07-30T02:50:11.912615Z","shell.execute_reply.started":"2024-07-30T02:50:11.902087Z","shell.execute_reply":"2024-07-30T02:50:11.911611Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"IterableDataset({\n    features: ['idx', 'src', 'tgt'],\n    n_shards: 1\n})"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\n# Convert the IterableDataset to a list\nlimited_data_list = list(limited_data)\n\n# Create a Dataset from the list\nlimited_data = Dataset.from_list(limited_data_list)\n\n# Create a DatasetDict\ndataset_dict = DatasetDict({\"train\": limited_data})\n\n# Verify the first example to ensure conversion was successful\nprint(dataset_dict[\"train\"][0])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T02:50:11.913741Z","iopub.execute_input":"2024-07-30T02:50:11.914110Z","iopub.status.idle":"2024-07-30T02:51:06.982368Z","shell.execute_reply.started":"2024-07-30T02:50:11.914084Z","shell.execute_reply":"2024-07-30T02:51:06.981312Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{'idx': 0, 'src': 'Some 14 months later, the second calf is born.', 'tgt': 'சுமார் 14 மாதங்கள் கழித்து, இரண்டாம் கன்றை ஈனுகிறது.'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert the dataset to a Pandas DataFrame\ntrain_df = dataset_dict[\"train\"].to_pandas()\n\n# Rename the columns\ntrain_df = train_df.rename(columns={\"src\": \"en\", \"tgt\": \"ta\"})\n\n# Drop the 'idx' column if it is not needed\ntrain_df = train_df.drop(columns=[\"idx\"])\n\n# Display the first few rows to verify\ntrain_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T02:51:06.983802Z","iopub.execute_input":"2024-07-30T02:51:06.984185Z","iopub.status.idle":"2024-07-30T02:51:07.317834Z","shell.execute_reply.started":"2024-07-30T02:51:06.984151Z","shell.execute_reply":"2024-07-30T02:51:07.316795Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                  en  \\\n0     Some 14 months later, the second calf is born.   \n1  \"Senior advocate Kapil Sibal, who was appearin...   \n2                         This photo was taken then.   \n3  So far two rounds of the JWG meeting have been...   \n4  The life of the world is nothing but play and ...   \n\n                                                  ta  \n0  சுமார் 14 மாதங்கள் கழித்து, இரண்டாம் கன்றை ஈனு...  \n1  ‘காா்த்தி சிதம்பரம் எம். பி. யாக உள்ளதால் எங்க...  \n2              அதன்போது எடுக்கப்பட்ட புகைப்படம் இது.  \n3  இதுவரை இணைப் பணிக் குழு இரண்டுகட்ட பேச்சுவார்த...  \n4  உலக வாழ்க்கை வீணும் விளையாட்டுமேயன்றி வேறில்லை...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>ta</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Some 14 months later, the second calf is born.</td>\n      <td>சுமார் 14 மாதங்கள் கழித்து, இரண்டாம் கன்றை ஈனு...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"Senior advocate Kapil Sibal, who was appearin...</td>\n      <td>‘காா்த்தி சிதம்பரம் எம். பி. யாக உள்ளதால் எங்க...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This photo was taken then.</td>\n      <td>அதன்போது எடுக்கப்பட்ட புகைப்படம் இது.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>So far two rounds of the JWG meeting have been...</td>\n      <td>இதுவரை இணைப் பணிக் குழு இரண்டுகட்ட பேச்சுவார்த...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The life of the world is nothing but play and ...</td>\n      <td>உலக வாழ்க்கை வீணும் விளையாட்டுமேயன்றி வேறில்லை...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Prepare the dataset for the model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\ntrain_df, val_df = train_test_split(train_df, test_size=0.1)\n\n# Create Hugging Face Datasets from the DataFrames\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Combine into a DatasetDict\ndataset = DatasetDict({\n    'train': train_dataset,\n    'validation': val_dataset\n})","metadata":{"execution":{"iopub.status.busy":"2024-07-30T02:51:07.321064Z","iopub.execute_input":"2024-07-30T02:51:07.321444Z","iopub.status.idle":"2024-07-30T02:51:08.374601Z","shell.execute_reply.started":"2024-07-30T02:51:07.321418Z","shell.execute_reply":"2024-07-30T02:51:08.373626Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-30T02:51:08.375909Z","iopub.execute_input":"2024-07-30T02:51:08.376216Z","iopub.status.idle":"2024-07-30T02:51:08.382226Z","shell.execute_reply.started":"2024-07-30T02:51:08.376191Z","shell.execute_reply":"2024-07-30T02:51:08.381194Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['en', 'ta', '__index_level_0__'],\n        num_rows: 180000\n    })\n    validation: Dataset({\n        features: ['en', 'ta', '__index_level_0__'],\n        num_rows: 20000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load the Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"model_name = 'Helsinki-NLP/opus-mt-en-mul'\ntokenizer = MarianTokenizer.from_pretrained(model_name)\n\n# Define a function to tokenize the dataset\ndef tokenize_function(examples):\n    inputs = examples['en']\n    targets = examples['ta']\n    model_inputs = tokenizer(inputs, text_target=targets, truncation=True, padding=\"max_length\", max_length=128)\n    return model_inputs\n\n# Tokenize the datasets\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T02:51:08.383557Z","iopub.execute_input":"2024-07-30T02:51:08.383898Z","iopub.status.idle":"2024-07-30T02:52:33.033722Z","shell.execute_reply.started":"2024-07-30T02:51:08.383866Z","shell.execute_reply":"2024-07-30T02:52:33.032817Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ac5ee2500724cae91e161ba8598d849"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/790k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a6c6d7e4f01478ebaa8cec2f1dfb919"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/707k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac9600081da1421dafefd9895eba90bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfc4bd56e9e846d3a339386a5b6eb28a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af6a17d8f1664f3983493abd83fc74d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/180000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bc6c6bdc1474c52a8faaa41ee5cda4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52f76d247dfc4029a041cf15400e50d1"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-07-30T02:52:33.034986Z","iopub.execute_input":"2024-07-30T02:52:33.035306Z","iopub.status.idle":"2024-07-30T02:52:33.041129Z","shell.execute_reply.started":"2024-07-30T02:52:33.035278Z","shell.execute_reply":"2024-07-30T02:52:33.040151Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['en', 'ta', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 180000\n    })\n    validation: Dataset({\n        features: ['en', 'ta', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 20000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"import torch, os\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-07-30T02:52:33.042187Z","iopub.execute_input":"2024-07-30T02:52:33.042510Z","iopub.status.idle":"2024-07-30T02:52:33.073652Z","shell.execute_reply.started":"2024-07-30T02:52:33.042485Z","shell.execute_reply":"2024-07-30T02:52:33.072712Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Load the model\nmodel = MarianMTModel.from_pretrained(model_name)\nmodel.to(device)\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir='./results',\n    evaluation_strategy='epoch',\n    learning_rate=2e-5,\n    warmup_steps=500,  # Adjust warmup steps\n    gradient_accumulation_steps=2,  # Adjust gradient accumulation steps\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    save_total_limit=3,\n    predict_with_generate=True,\n    fp16=True\n)\n\n# Define the data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n# Define a function to compute metrics\nmetric = load_metric('sacrebleu', trust_remote_code=True)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Compute BLEU score\n    result = metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n    result = {\"bleu\": result[\"score\"]}\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    return result\n\n# Create the Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T02:52:33.075309Z","iopub.execute_input":"2024-07-30T02:52:33.075681Z","iopub.status.idle":"2024-07-30T02:52:59.239584Z","shell.execute_reply.started":"2024-07-30T02:52:33.075648Z","shell.execute_reply":"2024-07-30T02:52:59.238553Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2024-07-30 02:52:35.208144: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-30 02:52:35.208256: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-30 02:52:35.329916: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/310M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c1931d3bd174547a9e1987a6c509b47"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05e5610502344e9ba7c587ca6823ff31"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_34/136250485.py:29: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric('sacrebleu', trust_remote_code=True)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab6b9f489a534a6fa39e9af643d0fbf3"}},"metadata":{}}]},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T02:52:59.240891Z","iopub.execute_input":"2024-07-30T02:52:59.241748Z","iopub.status.idle":"2024-07-30T04:13:14.949903Z","shell.execute_reply.started":"2024-07-30T02:52:59.241711Z","shell.execute_reply":"2024-07-30T04:13:14.948890Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5625/5625 1:20:13, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.520300</td>\n      <td>0.487908</td>\n      <td>6.237019</td>\n      <td>31.652850</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5625, training_loss=0.6461056993272569, metrics={'train_runtime': 4814.6888, 'train_samples_per_second': 37.386, 'train_steps_per_second': 1.168, 'total_flos': 6101705687040000.0, 'train_loss': 0.6461056993272569, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Save the Finetuned Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"# Save directory\nsave_directory = './finetuned-opusmt-en-to-ta'\n\n# Save model and tokenizer\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)\n\nprint(f\"Model saved to {save_directory}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T04:20:29.080567Z","iopub.execute_input":"2024-07-30T04:20:29.081293Z","iopub.status.idle":"2024-07-30T04:20:29.882712Z","shell.execute_reply.started":"2024-07-30T04:20:29.081260Z","shell.execute_reply":"2024-07-30T04:20:29.881708Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n","output_type":"stream"},{"name":"stdout","text":"Model saved to ./finetuned-opusmt-en-to-ta\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load the finetuned model and check out the translation","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\n# Load your model and tokenizer\nmodel_name =save_directory\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Example English sentence to translate\nenglish_sentence = \"My name is Varsha\"\n\n# Tokenize the input sentence\ninputs = tokenizer(english_sentence, return_tensors=\"pt\").to(device)\n\n# Generate translation\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n\n# Decode the generated tokens\ntranslated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(f\"English: {english_sentence}\")\nprint(f\"Tamil Translation: {translated_sentence}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T04:20:29.884528Z","iopub.execute_input":"2024-07-30T04:20:29.884973Z","iopub.status.idle":"2024-07-30T04:20:32.110333Z","shell.execute_reply.started":"2024-07-30T04:20:29.884940Z","shell.execute_reply":"2024-07-30T04:20:32.109442Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"English: My name is Varsha\nTamil Translation: என் பெயர் வர்ஷா\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}