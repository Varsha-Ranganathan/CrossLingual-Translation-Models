{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Setup","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets peft torch sacrebleu --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-01T03:28:27.218332Z","iopub.execute_input":"2024-08-01T03:28:27.218935Z","iopub.status.idle":"2024-08-01T03:28:42.131723Z","shell.execute_reply.started":"2024-08-01T03:28:27.218900Z","shell.execute_reply":"2024-08-01T03:28:42.130872Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50Tokenizer, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom datasets import load_dataset, load_metric\nimport numpy as np\nimport torch, os","metadata":{"execution":{"iopub.status.busy":"2024-08-01T03:28:42.133433Z","iopub.execute_input":"2024-08-01T03:28:42.133719Z","iopub.status.idle":"2024-08-01T03:28:59.634408Z","shell.execute_reply.started":"2024-08-01T03:28:42.133693Z","shell.execute_reply":"2024-08-01T03:28:59.633409Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-08-01 03:28:48.560048: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-01 03:28:48.560192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-01 03:28:48.687336: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-08-01T03:28:59.641947Z","iopub.execute_input":"2024-08-01T03:28:59.642274Z","iopub.status.idle":"2024-08-01T03:28:59.695316Z","shell.execute_reply.started":"2024-08-01T03:28:59.642242Z","shell.execute_reply":"2024-08-01T03:28:59.694228Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"raw_dataset = load_dataset('ai4bharat/samanantar', 'ta', split='train[:10000]', trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T03:28:59.696650Z","iopub.execute_input":"2024-08-01T03:28:59.696964Z","iopub.status.idle":"2024-08-01T03:38:41.926492Z","shell.execute_reply.started":"2024-08-01T03:28:59.696938Z","shell.execute_reply":"2024-08-01T03:38:41.925719Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/3.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"231be050c3954ccf92999b2bb22bddce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/6.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fd60d44eea24c4984e98a80613196ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fec03fed2bd4611b5c461b56c601af1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cd0464aa3704203885039e59014e9a9"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = raw_dataset.take(10000)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T03:38:41.927540Z","iopub.execute_input":"2024-08-01T03:38:41.927824Z","iopub.status.idle":"2024-08-01T03:38:41.935406Z","shell.execute_reply.started":"2024-08-01T03:38:41.927801Z","shell.execute_reply":"2024-08-01T03:38:41.934501Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-01T03:38:41.936587Z","iopub.execute_input":"2024-08-01T03:38:41.936892Z","iopub.status.idle":"2024-08-01T03:38:41.945724Z","shell.execute_reply.started":"2024-08-01T03:38:41.936858Z","shell.execute_reply":"2024-08-01T03:38:41.944887Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['idx', 'src', 'tgt'],\n    num_rows: 10000\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"### Prepare the dataset for the model","metadata":{}},{"cell_type":"code","source":"tokenized_datasets = dataset.train_test_split(test_size=0.1)\ntrain_dataset = tokenized_datasets['train']\neval_dataset = tokenized_datasets['test']","metadata":{"execution":{"iopub.status.busy":"2024-08-01T03:38:41.947271Z","iopub.execute_input":"2024-08-01T03:38:41.947558Z","iopub.status.idle":"2024-08-01T03:38:41.974170Z","shell.execute_reply.started":"2024-08-01T03:38:41.947536Z","shell.execute_reply":"2024-08-01T03:38:41.973347Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Load the Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\ntokenizer = MBart50Tokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T03:38:41.977041Z","iopub.execute_input":"2024-08-01T03:38:41.977309Z","iopub.status.idle":"2024-08-01T03:39:05.676786Z","shell.execute_reply.started":"2024-08-01T03:38:41.977286Z","shell.execute_reply":"2024-08-01T03:39:05.675790Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed1f38df7a1146d1aedd878ea70189b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"559449ffa48d42459e9f7fd7b5b16df4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a0a8680c0d4fdd9ba84c9b675a46b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d313cd4c501c4f3dbb4c73e00183cff7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5a391e800904c77855fdd1e4c4fdac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcce6002328d414a84b7003890489863"}},"metadata":{}}]},{"cell_type":"code","source":"# Define a function to tokenize the dataset\ndef preprocess_function(examples):\n    inputs = [ex for ex in examples['src']]\n    targets = [ex for ex in examples['tgt']]\n\n    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n\n    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n\n    labels = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels]\n    \n    model_inputs[\"labels\"] = labels\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-08-01T03:39:05.678259Z","iopub.execute_input":"2024-08-01T03:39:05.678566Z","iopub.status.idle":"2024-08-01T03:39:05.684719Z","shell.execute_reply.started":"2024-08-01T03:39:05.678540Z","shell.execute_reply":"2024-08-01T03:39:05.683739Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\ntokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T03:39:05.686162Z","iopub.execute_input":"2024-08-01T03:39:05.686439Z","iopub.status.idle":"2024-08-01T03:39:11.806680Z","shell.execute_reply.started":"2024-08-01T03:39:05.686416Z","shell.execute_reply":"2024-08-01T03:39:11.805734Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f08e8938db0443f182b69279ff45093f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca7d78e814ca44da8c48a14694fd2b34"}},"metadata":{}}]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=1,\n    predict_with_generate=True,\n    logging_dir='./logs',           # Directory for storing logs\n    logging_steps=10,               # Log every 10 steps\n    generation_max_length=128,      # Set maximum length for generation\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T03:39:11.807914Z","iopub.execute_input":"2024-08-01T03:39:11.808281Z","iopub.status.idle":"2024-08-01T03:39:11.933783Z","shell.execute_reply.started":"2024-08-01T03:39:11.808247Z","shell.execute_reply":"2024-08-01T03:39:11.932788Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"metric = load_metric(\"sacrebleu\")\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    result = metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-08-01T03:39:11.935118Z","iopub.execute_input":"2024-08-01T03:39:11.935740Z","iopub.status.idle":"2024-08-01T03:39:15.075492Z","shell.execute_reply.started":"2024-08-01T03:39:11.935704Z","shell.execute_reply":"2024-08-01T03:39:15.074728Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/4089552668.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric(\"sacrebleu\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2231d0746a0140d7b5f86ca62ddf4f2b"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for sacrebleu contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/sacrebleu.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"}]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_eval_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-01T03:39:15.076678Z","iopub.execute_input":"2024-08-01T03:39:15.077909Z","iopub.status.idle":"2024-08-01T03:39:15.960287Z","shell.execute_reply.started":"2024-08-01T03:39:15.077867Z","shell.execute_reply":"2024-08-01T03:39:15.959534Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"save_directory = \"/kaggle/new_model/saved_model\"\n\nos.makedirs(save_directory, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T03:39:15.961292Z","iopub.execute_input":"2024-08-01T03:39:15.961547Z","iopub.status.idle":"2024-08-01T03:39:15.966353Z","shell.execute_reply.started":"2024-08-01T03:39:15.961526Z","shell.execute_reply":"2024-08-01T03:39:15.965422Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T03:39:15.967503Z","iopub.execute_input":"2024-08-01T03:39:15.967817Z","iopub.status.idle":"2024-08-01T04:02:23.551780Z","shell.execute_reply.started":"2024-08-01T03:39:15.967787Z","shell.execute_reply":"2024-08-01T04:02:23.550894Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='563' max='563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [563/563 23:03, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Score</th>\n      <th>Counts</th>\n      <th>Totals</th>\n      <th>Precisions</th>\n      <th>Bp</th>\n      <th>Sys Len</th>\n      <th>Ref Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.233500</td>\n      <td>2.122425</td>\n      <td>9.092397</td>\n      <td>[3609, 1124, 467, 222]</td>\n      <td>[10078, 9078, 8083, 7122]</td>\n      <td>[35.81067672157174, 12.381581846221636, 5.7775578374365955, 3.1171019376579614]</td>\n      <td>0.961850</td>\n      <td>10078</td>\n      <td>10470</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"[3609, 1124, 467, 222]\" of type <class 'list'> for key \"eval/counts\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[10078, 9078, 8083, 7122]\" of type <class 'list'> for key \"eval/totals\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"[35.81067672157174, 12.381581846221636, 5.7775578374365955, 3.1171019376579614]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=563, training_loss=2.2975752730564367, metrics={'train_runtime': 1386.5509, 'train_samples_per_second': 6.491, 'train_steps_per_second': 0.406, 'total_flos': 2438020988928000.0, 'train_loss': 2.2975752730564367, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"### Save the Finetuned Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"# Save the trained model\ntrainer.save_model(save_directory)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T04:02:23.553005Z","iopub.execute_input":"2024-08-01T04:02:23.553278Z","iopub.status.idle":"2024-08-01T04:02:28.166637Z","shell.execute_reply.started":"2024-08-01T04:02:23.553254Z","shell.execute_reply":"2024-08-01T04:02:28.165673Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save directory\nsave_directory = './finetuned-MBart50-en-ta'\n\n# Save model and tokenizer\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)\n\nprint(f\"Model saved to {save_directory}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-01T04:02:28.168105Z","iopub.execute_input":"2024-08-01T04:02:28.168898Z","iopub.status.idle":"2024-08-01T04:02:34.176436Z","shell.execute_reply.started":"2024-08-01T04:02:28.168861Z","shell.execute_reply":"2024-08-01T04:02:34.175509Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"Model saved to ./finetuned-MBart50-en-ta\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Load the Finetuned model and check out the translation","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\n\n# Load your model and tokenizer\nmodel_name =save_directory\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Example English sentence to translate\nenglish_sentence = \"I am trying to translate this to another language\"\n\n# Tokenize the input sentence\ninputs = tokenizer(english_sentence, return_tensors=\"pt\").to(device)\n\n# Generate translation\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n\n# Decode the generated tokens\ntranslated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(f\"English: {english_sentence}\")\nprint(f\"Tamil Translation: {translated_sentence}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-01T04:03:02.727524Z","iopub.execute_input":"2024-08-01T04:03:02.728169Z","iopub.status.idle":"2024-08-01T04:03:12.017961Z","shell.execute_reply.started":"2024-08-01T04:03:02.728140Z","shell.execute_reply":"2024-08-01T04:03:12.017068Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"English: I am trying to translate this to another language\nTamil Translation: இதை வேறு மொழியில் மொழிபெயர்க்க முயற்சிக்கிறேன்\n","output_type":"stream"}]}]}